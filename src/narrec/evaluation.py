import json
import os
from collections import defaultdict
from typing import List

import matplotlib.pyplot as plt
import pandas as pd
import pytrec_eval

from narrec.benchmark.benchmark import Benchmark
from narrec.config import RESULT_DIR
from narrec.run_config import BENCHMARKS, FIRST_STAGES

METRICS = {
    'recall_1000',
    'ndcg_cut_10',
    'ndcg_cut_20',
    'ndcg_cut_100',
    'bpref',
    'map_cut_1000',
    'P_10',
    'P_20',
    'P_100'
}

RESULT_MEASURES = {
    'recall_1000': 'Recall@1000',
    'ndcg_cut_10': 'nDCG@10',
    'ndcg_cut_20': 'nDCG@20',
    'ndcg_cut_100': 'nDCG@100',
    #   'map': 'MAP',
    #   'bpref': 'bpref',
    'P_10': 'P@10',
    'P_20': 'P@20',
    "P_100": 'P@100'
}


def extract_run(run: dict, measure: str):
    run = sorted(run.items(), key=lambda x: int(x[0]))
    indices = [k for k, _ in run]
    values = [(v[measure],) for _, v in run]

    return {i: v[0] for i, v in zip(indices, values)}


def calculate_table_data(measures: List[tuple], results: List, relevant_topics: set):
    # calculate the mean scores of the given measures for each ranker
    max_m = {m[0]: 0.0 for m in measures}
    score_rows = defaultdict(dict)
    for name, raw_run in results:
        s_row = dict()
        for measure, _ in measures:
            run = extract_run(raw_run, measure)
            # add missing scores
            for q in relevant_topics.difference(set(run.keys())):
                run.update({q: 0.0})
            score = round(sum(run.values()) / len(run.keys()), 2)
            max_m[measure] = max(max_m[measure], score)
            s_row[measure] = score
        score_rows[name] = s_row
    return score_rows, max_m


def generate_diagram(input_json: str, output_dir: str):
    with open(input_json, 'r') as file:
        results = json.load(file)

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    for metric in METRICS:
        data = {run_name: res[metric] for run_name, res in results.items() if metric in res}
        df = pd.DataFrame(data)
        plt.figure()
        df.plot.bar(xlabel='Topic', ylabel=metric, figsize=(40, 10), width=1.0, edgecolor='black')
        plt.savefig(os.path.join(output_dir, f"{metric}.svg"), format="svg")
        plt.close('all')


def perform_evaluation_for_run(qrel, run_file: str):
    """
    Applys pytrec_eval to our given scenario
    Computes the previously defined metrics
    :param qrel: gold run file (given by benchmark)
    :param run_file: our actual produced run file by some method
    :return: None
    """

    print(f'Loading run file: {run_file}')
    with open(run_file, 'r') as f_run:
        run = pytrec_eval.parse_run(f_run)

    evaluator = pytrec_eval.RelevanceEvaluator(qrel, METRICS)
    return evaluator.evaluate(run)


def perform_evaluation(benchmark: Benchmark):
    print(f'Loading qrel file: {benchmark.get_qrel_path()}')
    with open(benchmark.qrel_path, 'r') as f_qrel:
        qrel = pytrec_eval.parse_qrel(f_qrel)

    relevant_topics = {q for q in qrel}
    print(f'{len(relevant_topics)} relevant topics identified')

    results = list()
    for run_name in FIRST_STAGES:
        run_path = os.path.join(RESULT_DIR, f'{run_name}.txt')
        results.append([run_name, perform_evaluation_for_run(qrel, run_path)])

    measures = [(k, v) for k, v in RESULT_MEASURES.items()]
    score_rows, max_m = calculate_table_data(measures, results, relevant_topics)
    print("--" * 60)
    print("Creating table content")
    print("--" * 60)
    # create tabular LaTeX code
    rows = []
    rows.append("% begin autogenerated")
    rows.append("\\toprule")
    rows.append(" & ".join(["First Stage", *(str(m[1]) for m in measures)]) + " \\\\")

    for name, scores in score_rows.items():
        row = f"{name.replace('FirstStage', '').split('-')[0].replace('_', ' ')} & "
        row += " & ".join(
            f"\\textbf{{{str(s)}}}" if max_m[m] == s else str(s) for m, s in scores.items())
        row += " \\\\"
        rows.append(row)

    rows.append("\\bottomrule")
    rows.append("% end autogenerated")

    print("\n".join(rows))
    print("--" * 60)

    print('\n\n\n')



def main():
    for benchmark in BENCHMARKS:
        perform_evaluation(benchmark)

if __name__ == '__main__':
    main()
